from typing import List
import tensorflow as tf
from copy import deepcopy
import tensorflow_recommenders as tfrs

# Third-party
from src.model.embedding import Embedding

class Tower(tf.keras.Model):

    def __init__(
        self,
        embedding_model: Embedding,
        cross_layer_projection_dim: int = None,
        dense_layers: List[int] = [],
    ) -> 'Tower':
        """
            Tower Model.
            
            Parameters:
                - embedding_model (tf.keras.Model): a model that transforms inputs to embeddings.
                - cross_layer (tfrs.layers.dcn.Cross): a Cross layer to perform feature crossing. Defaults to `None`.
                - densdense_layerse_layers_sizes (tf.keras.Sequential): a sequence of layers to perform deep feature interaction. Defaults to `[]`.
        """
        super().__init__()

        self._embedding_model = embedding_model

        self._cross_layer  = None
        self._dense_layers = []

        # The projection dimension determines the size of the feature 
        # space for learning cross-features. By employing low-rank 
        # techniques to approximate the weight matrices of the DCN, 
        # we can effectively reduce both training and serving costs.
        # More: https://www.tensorflow.org/recommenders/examples/dcn
        if cross_layer_projection_dim:
            self._cross_layer = tfrs.layers.dcn.Cross(
                projection_dim     = cross_layer_projection_dim,
                kernel_initializer = "glorot_uniform"
            )


        # Larger and more complex models, though often delivering better 
        # performance, generally require careful tuning. Nevertheless, it's 
        # essential to include at least one dense layer in the tower to unify 
        # the outputs from the embedding layers. This is necessary because 
        # each tower may generate embeddings of varying sizes, depending on 
        # the different features it processes.
        # More: https://www.tensorflow.org/recommenders/examples/deep_recommenders
        self._dense_layers = tf.keras.Sequential(
            # With ReLU activation
            [
                tf.keras.layers.Dense(layer_size, activation="relu")
                for layer_size in dense_layers[:-1]
            ] +
            # Without activation
            [
                tf.keras.layers.Dense(layer_size)
                for layer_size in dense_layers[-1:]
            ]
        )


    def call(
        self,
        inputs: tf.Tensor,
    ) -> tf.Tensor:
        """
            Calls the model on the input tensor.

            Parameters:
                - inputs (tf.Tensor): A tensor containing input data to be processed by the embedding model. 

            Returns:
                - (tf.Tensor): The output tensor generated by the model. 
        """
        x = self._embedding_model(inputs)
        x = self._cross_layer(x) if self._cross_layer else x
        x = self._dense_layers(x) if self._dense_layers else x
        return x
