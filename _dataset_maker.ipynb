{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset size (options: 100k, 1m, 10m, 20m, 25m)\n",
    "DATASET_SIZE: str = '1m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to construct dataset \"movielens\", builder_kwargs \"{'config': '10m-ratings', 'data_dir': None}\": BuilderConfig 10m-ratings not found with version None. Available: ['25m-ratings', '25m-movies', 'latest-small-ratings', 'latest-small-movies', '100k-ratings', '100k-movies', '1m-ratings', '1m-movies', '20m-ratings', '20m-movies']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ratings_dataset, ratings_dataset_info \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmovielens/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATASET_SIZE\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-ratings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# MovieLens dataset is not splitted into `train` and `test` sets by default.\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# So TFDS has put it all into `train` split. We load it completely and split\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# it manually.\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# `with_info=True` makes the `load` function return a `tfds.core.DatasetInfo`\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# object containing dataset metadata like version, description, homepage,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# citation, etc.\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Convert the tf.data.DataFrame into a DataFrame.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m df \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mas_dataframe(ratings_dataset, ratings_dataset_info)\n",
      "File \u001b[0;32m~/workspace/github/two-tower-recommender/venv/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/workspace/github/two-tower-recommender/venv/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:641\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;129m@tfds_logging\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    511\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m ):\n\u001b[1;32m    527\u001b[0m   \u001b[38;5;66;03m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03m  `tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m      Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# fmt: skip\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m   dbuilder \u001b[38;5;241m=\u001b[39m \u001b[43m_fetch_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtry_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m   _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)\n\u001b[1;32m    649\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/github/two-tower-recommender/venv/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:496\u001b[0m, in \u001b[0;36m_fetch_builder\u001b[0;34m(name, data_dir, builder_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m   builder_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_gcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/github/two-tower-recommender/venv/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/workspace/github/two-tower-recommender/venv/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:220\u001b[0m, in \u001b[0;36mbuilder\u001b[0;34m(name, try_gcs, **builder_kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m py_utils\u001b[38;5;241m.\u001b[39mtry_reraise(\n\u001b[1;32m    218\u001b[0m       prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to construct \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_dataset_repr()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    219\u001b[0m   ):\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pytype: disable=not-instantiable\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# If neither the code nor the files are found, raise DatasetNotFoundError\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m not_found_error\n",
      "File \u001b[0;32m~/workspace/github/two-tower-recommender/venv/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:289\u001b[0m, in \u001b[0;36mbuilder_init.<locals>.decorator\u001b[0;34m(function, dsbuilder, args, kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m _thread_id_to_builder_init_count[metadata\u001b[38;5;241m.\u001b[39mthread_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/workspace/github/two-tower-recommender/venv/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:1370\u001b[0m, in \u001b[0;36mFileReaderBuilder.__init__\u001b[0;34m(self, file_format, **kwargs)\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;129m@tfds_logging\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_init()\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1355\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1359\u001b[0m ):\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Initializes an instance of FileReaderBuilder.\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \n\u001b[1;32m   1362\u001b[0m \u001b[38;5;124;03m  Callers must pass arguments as keyword arguments.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;124;03m    **kwargs: Arguments passed to `DatasetBuilder`.\u001b[39;00m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1370\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mset_file_format(file_format)\n",
      "File \u001b[0;32m~/workspace/github/two-tower-recommender/venv/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:289\u001b[0m, in \u001b[0;36mbuilder_init.<locals>.decorator\u001b[0;34m(function, dsbuilder, args, kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m _thread_id_to_builder_init_count[metadata\u001b[38;5;241m.\u001b[39mthread_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/workspace/github/two-tower-recommender/venv/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:279\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, data_dir, config, version)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    276\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir, config\u001b[38;5;241m=\u001b[39mconfig, version\u001b[38;5;241m=\u001b[39mversion\n\u001b[1;32m    277\u001b[0m )\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# To do the work:\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_builder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_builder_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# Extract code version (VERSION or config)\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pick_version(version)\n",
      "File \u001b[0;32m~/workspace/github/two-tower-recommender/venv/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:1290\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, builder_config, version)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     builder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1289\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m builder_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilderConfig \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m not found with version \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Available: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;241m%\u001b[39m (name, version, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m   1293\u001b[0m     )\n\u001b[1;32m   1294\u001b[0m name \u001b[38;5;241m=\u001b[39m builder_config\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name:\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to construct dataset \"movielens\", builder_kwargs \"{'config': '10m-ratings', 'data_dir': None}\": BuilderConfig 10m-ratings not found with version None. Available: ['25m-ratings', '25m-movies', 'latest-small-ratings', 'latest-small-movies', '100k-ratings', '100k-movies', '1m-ratings', '1m-movies', '20m-ratings', '20m-movies']"
     ]
    }
   ],
   "source": [
    "ratings_dataset, ratings_dataset_info = tfds.load(\n",
    "    name = f'movielens/{DATASET_SIZE}-ratings',\n",
    "    # MovieLens dataset is not splitted into `train` and `test` sets by default.\n",
    "    # So TFDS has put it all into `train` split. We load it completely and split\n",
    "    # it manually.\n",
    "    split = 'train',\n",
    "    # `with_info=True` makes the `load` function return a `tfds.core.DatasetInfo`\n",
    "    # object containing dataset metadata like version, description, homepage,\n",
    "    # citation, etc.\n",
    "    with_info = True\n",
    ")\n",
    "\n",
    "# Convert the tf.data.DataFrame into a DataFrame.\n",
    "df = tfds.as_dataframe(ratings_dataset, ratings_dataset_info)\n",
    "\n",
    "# Convert byte values to strings.\n",
    "df = df.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the ratings with only the relevant columns.\n",
    "# Columns: user_id, movie_id, timestamp, user_rating\n",
    "\n",
    "ratings_df_columns = [\n",
    "    'user_id',\n",
    "    'movie_id',\n",
    "    'timestamp',\n",
    "    'user_rating',\n",
    "]\n",
    "ratings_df = df.loc[:, ratings_df_columns]\n",
    "\n",
    "# Remove duplicates\n",
    "ratings_df.drop_duplicates(inplace=True)\n",
    "\n",
    "ratings_df.to_parquet(f'data/{DATASET_SIZE}-ratings.parquet', compression='brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many of the movie titles in the dataset contain their release years.\n",
    "# Extracting this detail and treating it as a separate feature is more \n",
    "# valuable than leaving it combined with the title.\n",
    "\n",
    "movies_df_columns = [\n",
    "    'movie_id',\n",
    "    'movie_title',\n",
    "    'movie_genres',\n",
    "]\n",
    "movies_df = df.loc[:, movies_df_columns]\n",
    "\n",
    "# Extract the release years into a separate column.\n",
    "movies_df['movie_release_year'] = movies_df['movie_title'].str.extract(r'\\((\\d{4})\\)')\n",
    "\n",
    "# Remove the release years from the movie titles.\n",
    "movies_df['movie_title'] = movies_df['movie_title'].str.replace(r'\\s*\\(\\d{4}\\)\\s*', '', regex=True)\n",
    "\n",
    "movies_df['movie_genres'] = movies_df['movie_genres'].apply(tuple)  # Convert the genres into a tuple.\n",
    "\n",
    "# Remove duplicates\n",
    "movies_df.drop_duplicates(inplace=True)\n",
    "\n",
    "movies_df.to_parquet(f'data/{DATASET_SIZE}-movies.parquet', compression='brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the user features into a separate DataFrame.\n",
    "# Columns: user_id, user_gender, user_raw_age, user_zip_code,\n",
    "# user_bucketized_age, user_occupation_text, user_occupation_label\n",
    "\n",
    "users_df_columns = [\n",
    "    'user_id',\n",
    "    'user_gender',\n",
    "    # 'raw_user_age',\n",
    "    'user_zip_code',\n",
    "    'bucketized_user_age',\n",
    "    # 'user_occupation_text',\n",
    "    'user_occupation_label',\n",
    "]\n",
    "users_df = df.loc[:, users_df_columns]\n",
    "\n",
    "users_df.rename(\n",
    "    columns = {\n",
    "        'bucketized_user_age': 'user_bucketized_age'\n",
    "    },\n",
    "    inplace = True\n",
    ")\n",
    "\n",
    "users_df['user_gender'] = users_df['user_gender'].apply(lambda x: int(x))  # Cast booleans to integers\n",
    "\n",
    "# Remove duplicates\n",
    "users_df.drop_duplicates(inplace=True)\n",
    "\n",
    "users_df.to_parquet(f'data/{DATASET_SIZE}-users.parquet', compression='brotli')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
